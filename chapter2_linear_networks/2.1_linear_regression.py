# -*-coding:utf-8-*-
import random
import torch
from torch.utils import data
from torch import nn #nn:ç¥ç»ç½‘ç»œç¼©å†™
import numpy as np
from d2l import torch as d2l

#æ„é€ äººé€ æ•°æ®é›†
def synthetic_data(w,b,num_examples):
    """ç”Ÿæˆy = Xw + b + å™ªå£°"""
    '''
    ä½¿ç”¨çº¿æ€§æ¨¡å‹å‚æ•° ğ°=[2,âˆ’3.4]âŠ¤ ã€ ğ‘=4.2 
    å’Œå™ªå£°é¡¹ ğœ– ç”Ÿæˆæ•°æ®é›†åŠå…¶æ ‡ç­¾ï¼š
    '''
    X = torch.normal(0,1,(num_examples, len(w)))    #num_examples * 2çš„çŸ©é˜µ
    y = torch.matmul(X,w) + b   #num_examples * 1çš„çŸ©é˜µ
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1,1))

#å‡½æ•°åŠŸèƒ½ï¼šæ•°æ®é›†è¯»å–
#è¾“å…¥ï¼šæ‰¹é‡å¤§å°batch_size, ç‰¹å¾çŸ©é˜µfeaturesï¼Œ æ ‡ç­¾å‘é‡labels
#è¾“å‡ºï¼šå¤§å°ä¸ºbatch_sizeçš„å°æ‰¹é‡æ ·æœ¬ï¼ˆç‰¹å¾+æ ‡ç­¾ï¼‰
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples)) #rangeç›¸å½“äºä»0åˆ°n-1è¿™äº›æ•°ï¼Œç„¶åè½¬æˆListæ ¼å¼
    random.shuffle(indices) #å°†ç´¢å¼•æ‰“ä¹±ï¼Œå¼ºå•Šï¼
    #ä¾‹å¦‚batch_sizeä¸º16çš„è¯ï¼Œç›¸å½“äº16ä¸ªä¸ºä¸€ç»„
    # iæ¯è½®å¾ªç¯æ­¥è¿›16æ¬¡ï¼Œå³æ¯è½®å¾ªç¯iä¸ºå½“å‰æ–°batchçš„é¦–é¡¹
    for i in range(0,num_examples,batch_size):
        #æ¯è½®å¾ªç¯ä¸­ä»listä¸­åˆ‡ç‰‡[i:i+16]ï¼Œä»è€Œå¾—åˆ°å½“å‰batch(ä¸€ç»„16ä¸ªæ•°æ®)
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
        #yieldå°±æ˜¯ return è¿”å›ä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”è®°ä½è¿™ä¸ªè¿”å›çš„ä½ç½®ï¼Œä¸‹æ¬¡è¿­ä»£å°±ä»è¿™ä¸ªä½ç½®åå¼€

#å®šä¹‰çº¿æ€§å›å½’æ¨¡å‹
def linreg(X, w, b):
    return torch.matmul(X, w) + b

#å®šä¹‰æŸå¤±å‡½æ•°(å‡æ–¹è¯¯å·®ï¼‰
def squared_loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

#å®šä¹‰ä¼˜åŒ–ç®—æ³•:å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
#paramsä¸ºæ¨¡å‹å‚æ•°wä¸bï¼Œlrä¸ºå­¦ä¹ ç‡Learning rate
def sgd(params, lr, batch_size):
    #æ›´æ–°æ—¶æ— éœ€è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()


def my_train():
    # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
    w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)

    # è®­ç»ƒå‚æ•°è®¾ç½®
    lr = 0.03  # default:0.03
    num_epochs = 3  # æ•´ä¸ªæ•°æ®æ‰«ä¸‰é
    net = linreg  # å¦‚æ­¤å¯ä»¥æ–¹ä¾¿åæœŸæ¢æˆä¸åŒçš„æ¨¡å‹
    loss = squared_loss
    batch_size = 10  # default:10

    # è®­ç»ƒ
    for epoch in range(num_epochs):
        for X, y in data_iter(batch_size, features, labels):
            l = loss(net(X, w, b), y)  # ç”±å°æ‰¹é‡è®¡ç®—æŸå¤±
            # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡
            # `l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œå¹¶ä»¥æ­¤è®¡ç®—å…³äº[`w`, `b`]çš„æ¢¯åº¦
            # ç”±æ­¤ä¼˜åŒ–æ—¶éœ€é™¤ä¸Šä¸€ä¸ª batch_size
            l.sum().backward()
            sgd([w, b], lr, batch_size)
        with torch.no_grad():
            train_l = loss(net(features, w, b), labels)
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
    print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
    print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')



#ä»¥ä¸‹ä¸ºä½¿ç”¨PyTorchå®ç°ï¼š

def load_array(data_arrays, batch_size, is_train=True):
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

def PyTorch_train():

    batch_size = 10
    #æ•°æ®è¯»å–
    PyTorch_data_iter = load_array((features, labels), batch_size)

    #ç½‘ç»œç»“æ„å®šä¹‰:å•å±‚ç¥ç»ç½‘ç»œï¼šä»…ä¸€å±‚å…¨è¿æ¥å±‚
    # nn.Linear ä¸­ç¬¬ä¸€ä¸ªæŒ‡å®šè¾“å…¥ç‰¹å¾å½¢çŠ¶ï¼Œå³ 2ï¼Œ
    # ç¬¬äºŒä¸ªæŒ‡å®šè¾“å‡ºç‰¹å¾å½¢çŠ¶ï¼Œè¾“å‡ºç‰¹å¾å½¢çŠ¶ä¸ºå•ä¸ªæ ‡é‡ï¼Œå› æ­¤ä¸º 1
    net = nn.Sequential(nn.Linear(2,1))


    #æ¨¡å‹å‚æ•°å®šä¹‰
    #é€šè¿‡_ ç»“å°¾çš„æ–¹æ³•å°†å‚æ•°æ›¿æ¢ï¼Œä»è€Œåˆå§‹åŒ–å‚æ•°
    net[0].weight.data.normal_(0, 0.01)
    net[0].bias.data.fill_(0)

    #æŸå¤±å‡½æ•°
    '''å¹³æ–¹ğ¿2èŒƒæ•°,é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè¿”å›[æ‰€æœ‰æ ·æœ¬]æŸå¤±çš„å¹³å‡å€¼ï¼ˆmeanï¼‰
    reductionå‚æ•°çš„ä¸åŒå–å€¼
    none: no reduction will be applied.
    mean: the sum of the output will be divided by the number of elements in the output.
    sum: the output will be summed    
    è‹¥reduction = 'sum',åˆ™åœ¨ä¼˜åŒ–æ—¶åº”é™¤ä¸Šæ ·æœ¬æ•°,æœ¬ä¾‹ä¸­æ¯æ¬¡ä¼ å…¥10ä¸ªæ ·æœ¬ï¼ˆbatch_sizeï¼‰,ç›¸å½“äºlr = lr/10
    '''
    loss = nn.MSELoss()

    #å®ä¾‹åŒ–SGD
    trainer = torch.optim.SGD(net.parameters(), lr=0.03)

    #è®­ç»ƒ
    num_epochs = 3
    for epoch in range(num_epochs):
        for X, y in PyTorch_data_iter:
            l = loss(net(X), y)
            trainer.zero_grad()
            l.backward() #ä¸ä»é›¶å®ç°ç›¸æ¯”ï¼Œå‡å€¼è®¡ç®—å·²åœ¨Losså‡½æ•°ä¸­å®Œæˆ
            trainer.step()
        l = loss(net(features), labels)
        print(f'epoch {epoch + 1}, loss {l:f}')

    w = net[0].weight.data
    print('wçš„ä¼°è®¡è¯¯å·®ï¼š', true_w - w.reshape(true_w.shape))
    b = net[0].bias.data
    print('bçš„ä¼°è®¡è¯¯å·®ï¼š', true_b - b)


if __name__ == "__main__":

    #æ•°æ®é›†ç”Ÿæˆæœ‰å…³å‚æ•°
    true_w = torch.tensor([2,-3.4])
    true_b = 4.2
    features, labels = synthetic_data(true_w,true_b,1000)
    '''
    features ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ä¸ªäºŒç»´æ•°æ®æ ·æœ¬ï¼Œ
    labels ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ç»´æ ‡ç­¾å€¼ï¼ˆä¸€ä¸ªæ ‡é‡)
    '''
    # print('features:', features[0], '\nlabel:', labels[0])
    '''
    é€šè¿‡ç”Ÿæˆç¬¬äºŒä¸ªç‰¹å¾ features[:, 1] å’Œ labels çš„æ•£ç‚¹å›¾
    å¯ä»¥ç›´è§‚åœ°è§‚å¯Ÿåˆ°ä¸¤è€…ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚
    '''
    # d2l.set_figsize()
    # d2l.plt.scatter(features[:, (1)].detach().numpy(),
    #                 labels.detach().numpy(), 1);

    #ä»é›¶å¼€å§‹å®ç°
    my_train()

    #torchå®ç°
    PyTorch_train()

    #Q:
    # 1.ä¼¼ä¹epochç›¸åŒçš„æƒ…å†µä¸‹ï¼Œä»é›¶å¼€å§‹å®ç°æ•ˆæœè¦å¥½äº›ï¼Ÿ

    # 2.loss function reduction= 'mean' , lr=0.03:
    # epoch 1, loss 0.000361
    # epoch 2, loss 0.000095
    # epoch 3, loss 0.000095
    # wçš„ä¼°è®¡è¯¯å·®ï¼š tensor([-0.0002, -0.0002])
    # bçš„ä¼°è®¡è¯¯å·®ï¼š tensor([8.6784e-05])
    #
    # loss function reduction = 'sum', lr =0.03/batch_size=0.003
    # epoch 1, loss 0.176310
    # epoch 2, loss 0.091707
    # epoch 3, loss 0.092358
    # wçš„ä¼°è®¡è¯¯å·®ï¼š tensor([0.0007, 0.0012])
    # bçš„ä¼°è®¡è¯¯å·®ï¼š tensor([0.0005])
    #è¿™ä¸¤ç§æ–¹å¼ç†è®ºä¸Šç­‰ä»·ï¼Œä¸ºä½•loss functionå–meanæ˜¾è‘—ç”±äºå–sumçš„æ–¹å¼